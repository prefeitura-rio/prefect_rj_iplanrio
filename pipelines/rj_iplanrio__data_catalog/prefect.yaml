name: rj_iplanrio__data_catalog
prefect-version: 3.4.3

build:
  - prefect.deployments.steps.run_shell_script:
      id: get-commit-hash
      script: git rev-parse --short HEAD
      stream_output: false
  - prefect_docker.deployments.steps.build_docker_image:
      id: build-image
      requires: prefect-docker>=0.6.5
      image_name: ghcr.io/prefeitura-rio/prefect_rj_iplanrio/deployments
      tag: "rj_iplanrio__data_catalog-{{ get-commit-hash.stdout }}"
      dockerfile: pipelines/rj_iplanrio__data_catalog/Dockerfile

push:
  - prefect_docker.deployments.steps.push_docker_image:
      requires: prefect-docker>=0.6.5
      image_name: "{{ build-image.image_name }}"
      tag: "{{ build-image.tag }}"

pull:
  - prefect.deployments.steps.set_working_directory:
      directory: /opt/prefect/prefect_rj_iplanrio

deployments:
  - name: rj-iplanrio--data_catalog--staging
    version: "{{ build-image.tag }}"
    entrypoint: pipelines/rj_iplanrio__data_catalog/flow.py:rj_iplanrio__data_catalog
    work_pool:
      name: k3s-pool
      work_queue_name: default
      job_variables:
        image: "{{ build-image.image_name }}:{{ build-image.tag }}"
        command: uv run --package rj_iplanrio__data_catalog -- prefect flow-run execute
        secretName: prefect-jobs-secrets
        image_pull_policy: Always
  - name: rj-iplanrio--data_catalog--prod
    version: "{{ get-commit-hash.stdout }}"
    entrypoint: pipelines/rj_iplanrio__data_catalog/flow.py:rj_iplanrio__data_catalog
    work_pool:
      name: k3s-pool
      work_queue_name: default
      job_variables:
        image: "{{ build-image.image_name }}:{{ build-image.tag }}"
        command: uv run --package rj_iplanrio__data_catalog -- prefect flow-run execute
        secretName: prefect-jobs-secrets
        image_pull_policy: Always

    schedules:
    # Exemplo de schedule overwrite
    - interval: 86400 # executa a cada 24h
      anchor_date: "2022-01-01T01:00:00" # Data de inicio do schedule em UTC
      timezone: America/Sao_Paulo
      slug: slug # slug do schedule
      parameters:
        table_id: table_id # Nome da tabela no BigQuery
        execute_query: |
          SELECT
            column1,
            column2,
            column3
          FROM database.table_name
    # Exemplo de schedule incremental
    - interval: 86400
      anchor_date: "2022-01-01T02:32:00"
      timezone: America/Sao_Paulo
      slug: slug
      parameters:
        table_id: table_id
        dump_mode: append
        partition_columns: partition_column # Coluna de particionamento
        partition_date_format: "%Y-%m-%d" # Formato da data de particionamento
        break_query_frequency: day # Frequencia de particionamento
        break_query_start: current_day # Data de inicio do particionamento
        break_query_end: current_day # Data de fim do particionamento
        execute_query: |
          SELECT
            column1,
            column2,
            column3
          FROM database.table_name
